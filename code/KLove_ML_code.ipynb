{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text analysis of Lyrics and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Even If</td>\n",
       "      <td>MercyMe</td>\n",
       "      <td>Sometimes you lose some\\r\\nAnd right now, righ...</td>\n",
       "      <td>Inspiring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Broken Things</td>\n",
       "      <td>Matthew West</td>\n",
       "      <td>I stopped at the gate\\r\\nThinking I don't dese...</td>\n",
       "      <td>Inspiring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Old Church Choir</td>\n",
       "      <td>Zach Williams</td>\n",
       "      <td>Like a wildfire in my heart\\r\\nSunday morning,...</td>\n",
       "      <td>Inspiring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O Come to the Altar</td>\n",
       "      <td>Elevation Worship</td>\n",
       "      <td>Overwhelmed by the weight of your sin\\r\\nJesus...</td>\n",
       "      <td>Inspiring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Different</td>\n",
       "      <td>Micah Tyler</td>\n",
       "      <td>I don't wanna see anymore, give me a vision\\r\\...</td>\n",
       "      <td>Inspiring</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 title             artist  \\\n",
       "0              Even If            MercyMe   \n",
       "1        Broken Things       Matthew West   \n",
       "2     Old Church Choir      Zach Williams   \n",
       "3  O Come to the Altar  Elevation Worship   \n",
       "4            Different        Micah Tyler   \n",
       "\n",
       "                                              lyrics   Category  \n",
       "0  Sometimes you lose some\\r\\nAnd right now, righ...  Inspiring  \n",
       "1  I stopped at the gate\\r\\nThinking I don't dese...  Inspiring  \n",
       "2  Like a wildfire in my heart\\r\\nSunday morning,...  Inspiring  \n",
       "3  Overwhelmed by the weight of your sin\\r\\nJesus...  Inspiring  \n",
       "4  I don't wanna see anymore, give me a vision\\r\\...  Inspiring  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_df  = pd.read_excel(\"topsongs.xlsx\")\n",
    "music_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>109</td>\n",
       "      <td>76</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Kiss Me</td>\n",
       "      <td>Elevation Worship</td>\n",
       "      <td>Nightly, beside the green, green grass\\r\\nSwin...</td>\n",
       "      <td>Inspiring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          title             artist  \\\n",
       "count       113                113   \n",
       "unique      109                 76   \n",
       "top     Kiss Me  Elevation Worship   \n",
       "freq          3                  5   \n",
       "\n",
       "                                                   lyrics   Category  \n",
       "count                                                 113        113  \n",
       "unique                                                106          3  \n",
       "top     Nightly, beside the green, green grass\\r\\nSwin...  Inspiring  \n",
       "freq                                                    3         67  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Even If</td>\n",
       "      <td>MercyMe</td>\n",
       "      <td>Sometimes you lose some\\r\\nAnd right now, righ...</td>\n",
       "      <td>Inspiring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Broken Things</td>\n",
       "      <td>Matthew West</td>\n",
       "      <td>I stopped at the gate\\r\\nThinking I don't dese...</td>\n",
       "      <td>Inspiring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Old Church Choir</td>\n",
       "      <td>Zach Williams</td>\n",
       "      <td>Like a wildfire in my heart\\r\\nSunday morning,...</td>\n",
       "      <td>Inspiring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O Come to the Altar</td>\n",
       "      <td>Elevation Worship</td>\n",
       "      <td>Overwhelmed by the weight of your sin\\r\\nJesus...</td>\n",
       "      <td>Inspiring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Different</td>\n",
       "      <td>Micah Tyler</td>\n",
       "      <td>I don't wanna see anymore, give me a vision\\r\\...</td>\n",
       "      <td>Inspiring</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 title             artist  \\\n",
       "0              Even If            MercyMe   \n",
       "1        Broken Things       Matthew West   \n",
       "2     Old Church Choir      Zach Williams   \n",
       "3  O Come to the Altar  Elevation Worship   \n",
       "4            Different        Micah Tyler   \n",
       "\n",
       "                                              lyrics   category  \n",
       "0  Sometimes you lose some\\r\\nAnd right now, righ...  Inspiring  \n",
       "1  I stopped at the gate\\r\\nThinking I don't dese...  Inspiring  \n",
       "2  Like a wildfire in my heart\\r\\nSunday morning,...  Inspiring  \n",
       "3  Overwhelmed by the weight of your sin\\r\\nJesus...  Inspiring  \n",
       "4  I don't wanna see anymore, give me a vision\\r\\...  Inspiring  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_excel('train_music.xlsx')\n",
    "df_test = pd.read_excel('test_music.xlsx')\n",
    "\n",
    "X_train = df_train['lyrics'].values \n",
    "y_train = df_train['category'].values\n",
    "\n",
    "X_test = df_test['lyrics'].values \n",
    "y_test = df_test['category'].values\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Inspiring', 'inspiring_non_christian_music', 'non_inspiring']\n",
      "['Inspiring', 'inspiring_non_christian_music', 'non_inspiring']\n",
      "[0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "\n",
    "y_train_le = le.fit(y_train)\n",
    "y_test_le = le.fit(y_test)\n",
    "\n",
    "print(list(y_train_le.classes_))\n",
    "print(list(y_test_le.classes_))\n",
    "\n",
    "y_train = le.transform(y_train)\n",
    "y_test  = le.transform(y_test)\n",
    "\n",
    "print(y_train[:5])\n",
    "print(y_test[:-5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " \"a's\",\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"C://Users//datta//Movie_lens//stopwords_eng.txt\") as sw:\n",
    "    stopwords = sw.read().split()\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction: Word counts and Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabular size: 1555 words\n",
      "['yaks', 'yea', 'yeah', 'years', 'yesterday', 'yo', 'young', 'younger', 'youth', 'zeppelin']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "import re\n",
    "\n",
    "vec_porter = CountVectorizer(analyzer='word',\n",
    "                      decode_error='replace',\n",
    "                      tokenizer=lambda text: text.split(),\n",
    "                      preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()), \n",
    "                      stop_words=stopwords,) \n",
    "\n",
    "\n",
    "vec_porter.fit(X_train)\n",
    "\n",
    "print('Vocabular size: %s words' % len(vec_porter.vocabulary_))\n",
    "\n",
    "vocab_1 = vec_porter.get_feature_names()\n",
    "print(vocab_1[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabular size: 1325 words\n",
      "['ye', 'yea', 'yeah', 'year', 'yesterday', 'yo', 'young', 'younger', 'youth', 'zeppelin']\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "vec_porter = CountVectorizer(analyzer='word',\n",
    "                      decode_error='replace',\n",
    "                      tokenizer=lambda text: [porter.stem(word) for word in text.split()],\n",
    "                      preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()), \n",
    "                      stop_words=stopwords,) \n",
    "\n",
    "\n",
    "vec_porter.fit(X_train)\n",
    "\n",
    "print('Vocabular size: %s words' % len(vec_porter.vocabulary_))\n",
    "\n",
    "vocab_2 = vec_porter.get_feature_names()\n",
    "print(vocab_2[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive and negative words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative words: 4783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['zaps', 'zealot', 'zealous', 'zealously', 'zombie']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./negative-words.txt', 'r') as w:\n",
    "    negative_words = w.read().split()\n",
    "print('number of negative words: %s' % len(negative_words))\n",
    "negative_words[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of positive words: 2006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['youthful', 'zeal', 'zenith', 'zest', 'zippy']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./positive-words.txt', 'r') as w:\n",
    "    positive_words = w.read().split()\n",
    "print('number of positive words: %s' % len(positive_words))\n",
    "positive_words[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_words = set(negative_words)\n",
    "positive_words = set(positive_words)\n",
    "semantic_words = negative_words.union(positive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabular size: 313 words\n",
      "['wonderful', 'works', 'worn', 'worry', 'worse', 'worth', 'worthy', 'wow', 'wretch', 'wrong']\n"
     ]
    }
   ],
   "source": [
    "# With whitelist \n",
    "\n",
    "# With Porter Stemming\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "vec_porter = CountVectorizer(analyzer='word',\n",
    "                      decode_error='replace',\n",
    "                      tokenizer=lambda text: [word for word in text.split() if word in semantic_words],\n",
    "                      preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()), \n",
    "                      stop_words=stopwords,) \n",
    "\n",
    "\n",
    "vec_porter.fit(X_train)\n",
    "\n",
    "print('Vocabular size: %s words' % len(vec_porter.vocabulary_))\n",
    "\n",
    "vocab_3 = vec_porter.get_feature_names()\n",
    "print(vocab_3[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabular size: 272 words\n",
      "['wise', 'won', 'worn', 'worri', 'wors', 'worth', 'worthi', 'wow', 'wretch', 'wrong']\n"
     ]
    }
   ],
   "source": [
    "# With whitelist and Porter Stemming\n",
    "\n",
    "# With Porter Stemming\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "vec_porter = CountVectorizer(analyzer='word',\n",
    "                      decode_error='replace',\n",
    "                      tokenizer=lambda text: [porter.stem(word) for word in text.split() if word in semantic_words],\n",
    "                      preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()), \n",
    "                      stop_words=stopwords,) \n",
    "\n",
    "\n",
    "vec_porter.fit(X_train)\n",
    "\n",
    "print('Vocabular size: %s words' % len(vec_porter.vocabulary_))\n",
    "\n",
    "vocab_4 = vec_porter.get_feature_names()\n",
    "print(vocab_4[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary vocabulary sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words: 1555 \n",
      "Porter: 1325 \n",
      "Whitelist: 313 \n",
      "Porter + Whitelist: 272 \n"
     ]
    }
   ],
   "source": [
    "print('All words: %s ' % len(vocab_1))\n",
    "print('Porter: %s ' % len(vocab_2))\n",
    "print('Whitelist: %s ' % len(vocab_3))\n",
    "print('Porter + Whitelist: %s ' % len(vocab_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import pickle\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(semantic_words, open('./semantic_words.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "semantic_words = pickle.load(open('./semantic_words.p', 'rb'))\n",
    "\n",
    "with open('./stopwords_eng.txt') as sw:\n",
    "    stop_words = sw.read().split()\n",
    "    \n",
    "f1_scorer = metrics.make_scorer(metrics.f1_score, greater_is_better=True, pos_label=3, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = EnglishStemmer()\n",
    "\n",
    "# raw words\n",
    "tokenizer = lambda text: text.split()\n",
    "\n",
    "# words after Porter stemming \n",
    "tokenizer_porter = lambda text: [porter.stem(word) for word in text.split()]\n",
    "\n",
    "# Words after Snowball stemming\n",
    "tokenizer_snowball = lambda text: [snowball.stem(word) for word in text.split()]\n",
    "\n",
    "# Only words that are in a list of 'positive' or 'negative' words ('whitelist')\n",
    "# http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon\n",
    "tokenizer_whitelist = lambda text: [word for word in text.split() if word in semantic_words]\n",
    "\n",
    "# Porter-stemmed words in whitelist\n",
    "tokenizer_porter_wl = lambda text: [porter.stem(word) for word in text.split() if word in semantic_words]\n",
    "\n",
    "# Snowball-stemmed words in whitelist\n",
    "tokenizer_snowball_wl = lambda text: [snowball.stem(word) for word in text.split() if word in semantic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.622\n",
      "Best parameters set:\n",
      "\tvect__tokenizer: <function <lambda> at 0x000001D10BE5CA60>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    1.4s finished\n"
     ]
    }
   ],
   "source": [
    "pipeline_1 = Pipeline([\n",
    "    ('vect', CountVectorizer(binary=True,\n",
    "                             stop_words=stop_words,\n",
    "                             ngram_range=(1,1),\n",
    "                             preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                             )),\n",
    "    ('clf', MultinomialNB(fit_prior=False, \n",
    "                          alpha=1.0))\n",
    "])\n",
    "\n",
    "parameters_1 = dict(\n",
    "    vect__tokenizer=[tokenizer, tokenizer_porter, tokenizer_whitelist, tokenizer_porter_wl],\n",
    ")\n",
    "\n",
    "grid_search_1 = GridSearchCV(pipeline_1, \n",
    "                           parameters_1, \n",
    "                           n_jobs=1, \n",
    "                           verbose=1,\n",
    "                           scoring=f1_scorer,\n",
    "                           cv=3\n",
    "                )\n",
    "\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline_1.steps])\n",
    "grid_search_1.fit(X_train, y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search_1.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters_1 = grid_search_1.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters_1.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.615\n",
      "Best parameters set:\n",
      "\tvect__tokenizer: <function <lambda> at 0x000001D10BE5CA60>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1023: UserWarning: Note that pos_label (set to 3) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    1.3s finished\n"
     ]
    }
   ],
   "source": [
    "pipeline_2 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(binary=False,\n",
    "                             stop_words=stop_words,\n",
    "                             ngram_range=(1,1),\n",
    "                             preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                             )),\n",
    "    ('clf', MultinomialNB(fit_prior=False, \n",
    "                          alpha=1.0))\n",
    "])\n",
    "\n",
    "parameters_2 = dict(\n",
    "    vect__tokenizer=[tokenizer, tokenizer_porter, tokenizer_whitelist, tokenizer_porter_wl],\n",
    ")\n",
    "\n",
    "grid_search_2 = GridSearchCV(pipeline_2, \n",
    "                           parameters_2, \n",
    "                           n_jobs=1, \n",
    "                           verbose=1,\n",
    "                           scoring=f1_scorer,\n",
    "                           cv=None\n",
    "                )\n",
    "\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline_2.steps])\n",
    "grid_search_2.fit(X_train, y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search_2.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters_2 = grid_search_2.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters_2.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters_1[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from scipy import interp\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "vect_1 = CountVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "vect_2 = CountVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_porter)\n",
    "    \n",
    "vect_3 = CountVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_snowball)  \n",
    "\n",
    "vect_4 = CountVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_whitelist)  \n",
    "\n",
    "vect_5 = CountVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_porter_wl)\n",
    "\n",
    "vect_6 = CountVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_snowball_wl)\n",
    "\n",
    "vect_7 = TfidfVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "vect_8 = TfidfVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_porter)\n",
    "    \n",
    "vect_9 = TfidfVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_snowball)\n",
    "\n",
    "vect_10 = TfidfVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_whitelist)    \n",
    "\n",
    "vect_11 = TfidfVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_porter_wl)\n",
    "\n",
    "vect_12 = TfidfVectorizer(binary=False,\n",
    "                         stop_words=stop_words,\n",
    "                         ngram_range=(1,1),\n",
    "                         preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()),\n",
    "                         tokenizer=tokenizer_snowball_wl)\n",
    "\n",
    "pipelines = []\n",
    "vectorizers = [vect_1, vect_2, vect_3, vect_4, vect_5, vect_6, vect_7, vect_8, vect_9, vect_10, vect_11, vect_12]\n",
    "for v in vectorizers:\n",
    "    pipelines.append(Pipeline([('vect', v),\n",
    "                               ('clf', MultinomialNB(fit_prior=False, alpha=1.0))]))\n",
    "    \n",
    "for v in vectorizers[:6]:\n",
    "    pipelines.append(Pipeline([('vect', v),\n",
    "                               ('clf', BernoulliNB(fit_prior=False, alpha=1.0))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.53975, std: 0.05550, params: {'vect__tokenizer': <function <lambda> at 0x000001D10BE5C730>},\n",
       " mean: 0.46681, std: 0.04471, params: {'vect__tokenizer': <function <lambda> at 0x000001D10BE5CEA0>},\n",
       " mean: 0.62175, std: 0.08150, params: {'vect__tokenizer': <function <lambda> at 0x000001D10BE5CA60>},\n",
       " mean: 0.59565, std: 0.07379, params: {'vect__tokenizer': <function <lambda> at 0x000001D10BE5CAE8>}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_1.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import itertools\n",
    "from sklearn import datasets, svm, cross_validation, tree, preprocessing, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "music_df['merge'] = music_df['title'] + \" \" + music_df['artist'] + \" \" + music_df['lyrics']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Even If MercyMe Sometimes you lose some\\r\\nAnd...\n",
       "1    Broken Things Matthew West I stopped at the ga...\n",
       "2    Old Church Choir Zach Williams Like a wildfire...\n",
       "3    O Come to the Altar Elevation Worship Overwhel...\n",
       "4    Different Micah Tyler I don't wanna see anymor...\n",
       "Name: merge, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_df['merge'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113, 2373)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Inspiring': 0, 'inspiring_non_christian_music': 1, 'non_inspiring': 2}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer  = TfidfTransformer(smooth_idf=False)\n",
    "feature_cols = ['title','artist','lyrics']\n",
    "category = music_df['Category'].unique()\n",
    "category_dict = {value:index for index, value in enumerate(category)}\n",
    "corpus = music_df['merge']\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "print(X.shape)\n",
    "vectorizer.get_feature_names()\n",
    "y = music_df['Category'].map(category_dict)\n",
    "category_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 2 1 1 2 0 0 0 2 0 0 2 0 0 2 0 0 0 1 0 0 0 0 1 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred_class = mnb.predict(X_test)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.620689655172\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUll Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17\n",
       "2     7\n",
       "1     5\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the class distribution of the testing set using a Pandas Series Methods\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6551724137931034"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3448275862068966"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1  - y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null accuracy single line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.586207\n",
       "Name: Category, dtype: float64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts().head(1)/ len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compairing the true and predicted response values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true [2 0 0 0 2 0 0 1 0 0]\n",
      "Pred [0 0 0 2 1 1 2 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#print the first 25 true and predicted responses\n",
    "print ('true', y_test.values[0:10])\n",
    "print('Pred', y_pred_class[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.586207\n",
       "Name: Category, dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts().head(1) / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  2  2]\n",
      " [ 2  2  1]\n",
      " [ 3  1  3]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  2,  2],\n",
       "       [ 2,  2,  1],\n",
       "       [ 3,  1,  3]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_class)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAFBCAYAAAD3xy2CAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d10cab3f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "c = plt.summer()\n",
    "plt.colorbar(c)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
